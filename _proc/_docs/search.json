[
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "The Bayesian Approach and Gaussian Processes",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "linearregression.html",
    "href": "linearregression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\npath = Path('Data/homeprices.csv')\ndf = pd.read_csv(path)\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\nplt.xlabel('area')\nplt.ylabel('price')\nplt.scatter(df.area,df.price,color='red',marker='+')\n\nplt.show()\nnew_df = df.drop('price',axis='columns')\nnew_df = new_df.drop('bedrooms',axis='columns')\nnew_df = new_df.drop('age',axis='columns')\nnew_df\n\n\n\n\n\n\n\n\narea\n\n\n\n\n0\n2600\n\n\n1\n3000\n\n\n2\n3200\n\n\n3\n3600\n\n\n4\n4000\n\n\n5\n4100\nprice = df.price\nprice\n\n0    550000\n1    565000\n2    610000\n3    595000\n4    760000\n5    810000\nName: price, dtype: int64\nfrom sklearn import linear_model\n# Create linear regression object\nreg = linear_model.LinearRegression()\nreg.fit(new_df,price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nreg.predict([[3300]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([628813.88621022])\nreg.coef_\n\narray([167.30954677])\nreg.intercept_\n\n76692.3818707813\nY = m * X + b (m is coefficient and b is intercept)\n5000*reg.coef_ + reg.intercept_\n\narray([913240.11571842])\nreg.predict([[5000]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([913240.11571842])"
  },
  {
    "objectID": "linearregression.html#generate-csv-file-with-list-of-home-price-predictions",
    "href": "linearregression.html#generate-csv-file-with-list-of-home-price-predictions",
    "title": "Linear Regression",
    "section": "Generate CSV file with list of home price predictions",
    "text": "Generate CSV file with list of home price predictions"
  },
  {
    "objectID": "svr.html",
    "href": "svr.html",
    "title": "Support Vector Regression",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "kernals.html",
    "href": "kernals.html",
    "title": "Non-linear Input Transformations and Kernels",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Tools",
    "section": "",
    "text": "import pandas as pd\nfrom pivottablejs import pivot_ui\nimport ipypivot as pt\n\ndf = pd.read_csv('Data/salaries.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\nsalary_more_then_100k\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n0\n\n\n1\ngoogle\nsales executive\nmasters\n0\n\n\n2\ngoogle\nbusiness manager\nbachelors\n1\n\n\n3\ngoogle\nbusiness manager\nmasters\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n0\n\n\n\n\n\n\n\n\npivot_ui(df)"
  },
  {
    "objectID": "tools.html#pivottablesjs",
    "href": "tools.html#pivottablesjs",
    "title": "Tools",
    "section": "",
    "text": "import pandas as pd\nfrom pivottablejs import pivot_ui\nimport ipypivot as pt\n\ndf = pd.read_csv('Data/salaries.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\nsalary_more_then_100k\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n0\n\n\n1\ngoogle\nsales executive\nmasters\n0\n\n\n2\ngoogle\nbusiness manager\nbachelors\n1\n\n\n3\ngoogle\nbusiness manager\nmasters\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n0\n\n\n\n\n\n\n\n\npivot_ui(df)"
  },
  {
    "objectID": "tools.html#pytube",
    "href": "tools.html#pytube",
    "title": "Tools",
    "section": "pytube",
    "text": "pytube\n\nfrom pytube import YouTube\nyt = YouTube('https://www.youtube.com/watch?v=dQw4w9WgXcQ')\n\n\nyt.title\n\n'Never Gonna Give You Up'\n\n\n\nyt.thumbnail_url\n\n'https://i.ytimg.com/vi/dQw4w9WgXcQ/hq720.jpg?sqp=-oaymwEXCNUGEOADIAQqCwjVARCqCBh4INgESFo&rs=AOn4CLBX-HcaMSEAucUr5J0qD5nEyiPAoQ'\n\n\n\nhigh_yt = yt.streams.get_highest_resolution()\n\n\nhigh_yt.download(output_path = 'Data')\n\n'/home/ben/BENEDICT_Only/Benedict_Projects/Benedict_ML/ML/nbs/Data/Never Gonna Give You Up.mp4'\n\n\n\nfor streams in yt.streams:\n    print(streams)\n\n&lt;Stream: itag=\"17\" mime_type=\"video/3gpp\" res=\"144p\" fps=\"6fps\" vcodec=\"mp4v.20.3\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\"&gt;\n&lt;Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"25fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\"&gt;\n&lt;Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"25fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\"&gt;\n&lt;Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"25fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"25fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"25fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"25fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"25fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"25fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"&gt;\n&lt;Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\"&gt;\n&lt;Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\"&gt;\n&lt;Stream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"&gt;\n&lt;Stream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"&gt;\n&lt;Stream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"&gt;\n\n\n\nyt.streams\\\n.filter(progressive=True, file_extension='mp4')\\\n.order_by('resolution')\\\n.desc().first().download(output_path = 'Data')\n\n'/home/ben/BENEDICT_Only/Benedict_Projects/Benedict_ML/ML/nbs/Data/Never Gonna Give You Up.mp4'"
  },
  {
    "objectID": "tools.html#mito",
    "href": "tools.html#mito",
    "title": "Tools",
    "section": "mito",
    "text": "mito\n\nimport mitosheet\nmitosheet.sheet(analysis_to_replay=\"id-qdjqkelpwx\")\n\n\n\n        \n    \n\n\n\nfrom mitosheet.public.v3 import *; # Analysis Name:id-qdjqkelpwx;\nimport pandas as pd\n\n# Imported salaries.csv\nsalaries = pd.read_csv(r'/home/ben/BENEDICT_Only/Benedict_Projects/Benedict_ML/ML/nbs/Data/salaries.csv')\n\n# sort the column company in ascending order\nsalaries.sort_values('company', ascending=True, inplace=True)"
  },
  {
    "objectID": "tools.html#manim",
    "href": "tools.html#manim",
    "title": "Tools",
    "section": "manim",
    "text": "manim\n\nanimation library"
  },
  {
    "objectID": "tools.html#altair",
    "href": "tools.html#altair",
    "title": "Tools",
    "section": "Altair",
    "text": "Altair\n\nineractive plotting"
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Convolution Neutal Networks",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "hyperparameteroptimization.html",
    "href": "hyperparameteroptimization.html",
    "title": "Hyper Parameter Optimization",
    "section": "",
    "text": "For iris flower dataset in sklearn library, we are going to find out best model and best hyper parameters using GridSearchCV\nLoad iris flower dataset\nfrom sklearn import svm, datasets\niris = datasets.load_iris()\nimport pandas as pd\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf['flower'] = iris.target\ndf['flower'] = df['flower'].apply(lambda x: iris.target_names[x])\ndf[47:150]\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nflower\n\n\n\n\n47\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n48\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n49\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n50\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n103 rows × 5 columns"
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-1-use-train_test_split-and-manually-tune-parameters-by-trial-and-error",
    "href": "hyperparameteroptimization.html#approach-1-use-train_test_split-and-manually-tune-parameters-by-trial-and-error",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 1: Use train_test_split and manually tune parameters by trial and error",
    "text": "Approach 1: Use train_test_split and manually tune parameters by trial and error\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n\n\nmodel = svm.SVC(kernel='rbf',C=30,gamma='auto')\nmodel.fit(X_train,y_train)\nmodel.score(X_test, y_test)\n\n0.9555555555555556"
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-2-use-k-fold-cross-validation",
    "href": "hyperparameteroptimization.html#approach-2-use-k-fold-cross-validation",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 2: Use K Fold Cross validation",
    "text": "Approach 2: Use K Fold Cross validation\nManually try suppling models with different parameters to cross_val_score function with 5 fold cross validation\n\ncross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([1.        , 1.        , 0.9       , 0.96666667, 1.        ])\n\n\n\ncross_val_score(svm.SVC(kernel='rbf',C=10,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])\n\n\n\ncross_val_score(svm.SVC(kernel='rbf',C=20,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([0.96666667, 1.        , 0.9       , 0.96666667, 1.        ])\n\n\nAbove approach is tiresome and very manual. We can use for loop as an alternative\n\nkernels = ['rbf', 'linear']\nC = [1,10,20]\navg_scores = {}\nfor kval in kernels:\n    for cval in C:\n        cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),iris.data, iris.target, cv=5)\n        avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)\n\navg_scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'rbf_1': 0.9800000000000001,\n 'rbf_10': 0.9800000000000001,\n 'rbf_20': 0.9666666666666668,\n 'linear_1': 0.9800000000000001,\n 'linear_10': 0.9733333333333334,\n 'linear_20': 0.9666666666666666}\n\n\nFrom above results we can say that rbf with C=1 or 10 or linear with C=1 will give best performance\n\nApproach 3: Use GridSearchCV\n\nGridSearchCV does exactly same thing as for loop above but in a single line of code\n\nfrom sklearn.model_selection import GridSearchCV\nclf = GridSearchCV(svm.SVC(gamma='auto'), {\n    'C': [1,10,20],\n    'kernel': ['rbf','linear']\n}, cv=5, return_train_score=False)\nclf.fit(iris.data, iris.target)\nclf.cv_results_\n\n{'mean_fit_time': array([0.00118256, 0.00104566, 0.0007266 , 0.00084271, 0.00088511,\n        0.00065002]),\n 'std_fit_time': array([6.73577797e-04, 4.38131552e-04, 1.68876617e-04, 3.96551972e-04,\n        4.28567104e-04, 8.82425265e-05]),\n 'mean_score_time': array([0.00081396, 0.00048532, 0.00048227, 0.00054522, 0.00063434,\n        0.00044188]),\n 'std_score_time': array([4.28254909e-04, 1.10279724e-04, 1.84006103e-04, 1.59358051e-04,\n        3.89407295e-04, 6.20551122e-05]),\n 'param_C': masked_array(data=[1, 1, 10, 10, 20, 20],\n              mask=[False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'param_kernel': masked_array(data=['rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear'],\n              mask=[False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'params': [{'C': 1, 'kernel': 'rbf'},\n  {'C': 1, 'kernel': 'linear'},\n  {'C': 10, 'kernel': 'rbf'},\n  {'C': 10, 'kernel': 'linear'},\n  {'C': 20, 'kernel': 'rbf'},\n  {'C': 20, 'kernel': 'linear'}],\n 'split0_test_score': array([0.96666667, 0.96666667, 0.96666667, 1.        , 0.96666667,\n        1.        ]),\n 'split1_test_score': array([1., 1., 1., 1., 1., 1.]),\n 'split2_test_score': array([0.96666667, 0.96666667, 0.96666667, 0.9       , 0.9       ,\n        0.9       ]),\n 'split3_test_score': array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667,\n        0.93333333]),\n 'split4_test_score': array([1., 1., 1., 1., 1., 1.]),\n 'mean_test_score': array([0.98      , 0.98      , 0.98      , 0.97333333, 0.96666667,\n        0.96666667]),\n 'std_test_score': array([0.01632993, 0.01632993, 0.01632993, 0.03887301, 0.03651484,\n        0.0421637 ]),\n 'rank_test_score': array([1, 1, 1, 4, 5, 6], dtype=int32)}\n\n\n\ndf = pd.DataFrame(clf.cv_results_)\ndf\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_C\nparam_kernel\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.001183\n0.000674\n0.000814\n0.000428\n1\nrbf\n{'C': 1, 'kernel': 'rbf'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n1\n0.001046\n0.000438\n0.000485\n0.000110\n1\nlinear\n{'C': 1, 'kernel': 'linear'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n2\n0.000727\n0.000169\n0.000482\n0.000184\n10\nrbf\n{'C': 10, 'kernel': 'rbf'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n3\n0.000843\n0.000397\n0.000545\n0.000159\n10\nlinear\n{'C': 10, 'kernel': 'linear'}\n1.000000\n1.0\n0.900000\n0.966667\n1.0\n0.973333\n0.038873\n4\n\n\n4\n0.000885\n0.000429\n0.000634\n0.000389\n20\nrbf\n{'C': 20, 'kernel': 'rbf'}\n0.966667\n1.0\n0.900000\n0.966667\n1.0\n0.966667\n0.036515\n5\n\n\n5\n0.000650\n0.000088\n0.000442\n0.000062\n20\nlinear\n{'C': 20, 'kernel': 'linear'}\n1.000000\n1.0\n0.900000\n0.933333\n1.0\n0.966667\n0.042164\n6\n\n\n\n\n\n\n\n\ndf[['param_C','param_kernel','mean_test_score']]\n\n\n\n\n\n\n\n\nparam_C\nparam_kernel\nmean_test_score\n\n\n\n\n0\n1\nrbf\n0.980000\n\n\n1\n1\nlinear\n0.980000\n\n\n2\n10\nrbf\n0.980000\n\n\n3\n10\nlinear\n0.973333\n\n\n4\n20\nrbf\n0.966667\n\n\n5\n20\nlinear\n0.966667\n\n\n\n\n\n\n\n\nclf.best_params_\n\n{'C': 1, 'kernel': 'rbf'}\n\n\n\nclf.best_score_\n\n0.9800000000000001\n\n\n\ndir(clf)\n\n['__abstractmethods__',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__sklearn_clone__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_abc_impl',\n '_build_request_for_signature',\n '_check_feature_names',\n '_check_n_features',\n '_check_refit_for_multimetric',\n '_estimator_type',\n '_format_results',\n '_get_default_requests',\n '_get_metadata_request',\n '_get_param_names',\n '_get_tags',\n '_more_tags',\n '_parameter_constraints',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_required_parameters',\n '_run_search',\n '_select_best_index',\n '_validate_data',\n '_validate_params',\n 'best_estimator_',\n 'best_index_',\n 'best_params_',\n 'best_score_',\n 'classes_',\n 'cv',\n 'cv_results_',\n 'decision_function',\n 'error_score',\n 'estimator',\n 'fit',\n 'get_metadata_routing',\n 'get_params',\n 'inverse_transform',\n 'multimetric_',\n 'n_features_in_',\n 'n_jobs',\n 'n_splits_',\n 'param_grid',\n 'pre_dispatch',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'refit',\n 'refit_time_',\n 'return_train_score',\n 'score',\n 'score_samples',\n 'scorer_',\n 'scoring',\n 'set_fit_request',\n 'set_params',\n 'transform',\n 'verbose']\n\n\nUse RandomizedSearchCV to reduce number of iterations and with random combination of parameters. This is useful when you have too many parameters to try and your training time is longer. It helps reduce the cost of computation\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrs = RandomizedSearchCV(svm.SVC(gamma='auto'), {\n        'C': [1,10,20],\n        'kernel': ['rbf','linear']\n    }, \n    cv=5, \n    return_train_score=False, \n    n_iter=2\n)\nrs.fit(iris.data, iris.target)\npd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]\n\n\n\n\n\n\n\n\nparam_C\nparam_kernel\nmean_test_score\n\n\n\n\n0\n20\nrbf\n0.966667\n\n\n1\n1\nrbf\n0.980000\n\n\n\n\n\n\n\nHow about different models with different hyperparameters?\n\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {\n            'C': list(range(1, 21)),\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': list(range(1, 11))\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': list(range(1, 11))\n        }\n    },\n    'naive_bayes_gaussian': {\n        'model': GaussianNB(),\n        'params': {}\n    },\n    'naive_bayes_multinomial': {\n        'model': MultinomialNB(),\n        'params': {}\n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            \n        }\n    }     \n}\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(iris.data, iris.target)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf\n\n\n\n\n\n\n\n\nmodel\nbest_score\nbest_params\n\n\n\n\n0\nsvm\n0.986667\n{'C': 4, 'kernel': 'rbf'}\n\n\n1\nrandom_forest\n0.966667\n{'n_estimators': 4}\n\n\n2\nlogistic_regression\n0.966667\n{'C': 4}\n\n\n3\nnaive_bayes_gaussian\n0.953333\n{}\n\n\n4\nnaive_bayes_multinomial\n0.953333\n{}\n\n\n5\ndecision_tree\n0.966667\n{'criterion': 'gini'}\n\n\n\n\n\n\n\nBased on above, I can conclude that SVM with C=1 and kernel=‘rbf’ is the best model for solving my problem of iris flower classification"
  },
  {
    "objectID": "decisiontree.html",
    "href": "decisiontree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Simple to understand, interpret, visualize.\nDecision trees implicitly performvariable screening or feature selection.\nCan handle both numerical and categorical data. Can also handle multi-output problems.\nDecision trees require relatively little effort from users for data preparation.\nNonlinear relationships between parameters do not affect tree performance."
  },
  {
    "objectID": "decisiontree.html#advantages-of-cart",
    "href": "decisiontree.html#advantages-of-cart",
    "title": "Decision Tree",
    "section": "",
    "text": "Simple to understand, interpret, visualize.\nDecision trees implicitly performvariable screening or feature selection.\nCan handle both numerical and categorical data. Can also handle multi-output problems.\nDecision trees require relatively little effort from users for data preparation.\nNonlinear relationships between parameters do not affect tree performance."
  },
  {
    "objectID": "decisiontree.html#disadvantages-of-cart",
    "href": "decisiontree.html#disadvantages-of-cart",
    "title": "Decision Tree",
    "section": "Disadvantages of CART",
    "text": "Disadvantages of CART\n\nDecision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting.\nDecision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called variance, which needs to be lowered by methods like bagging and boosting.\nGreedy algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees, where the features and samples are randomly sampled with replacement.\nDecision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the data set prior to fitting with the decision tree.\n\n\nimport pandas as pd\nfrom fastai.vision.all import *\n\n\npath = Path('Data')\nname = 'salaries.csv'\n\n\ndf = pd.read_csv(path/name)\ndf.head()\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\nsalary_more_then_100k\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n0\n\n\n1\ngoogle\nsales executive\nmasters\n0\n\n\n2\ngoogle\nbusiness manager\nbachelors\n1\n\n\n3\ngoogle\nbusiness manager\nmasters\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n0\n\n\n\n\n\n\n\n\ninputs = df.drop('salary_more_then_100k',axis='columns')\n\n\ntarget = df['salary_more_then_100k']\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nle_company = LabelEncoder()\nle_job = LabelEncoder()\nle_degree = LabelEncoder()\n\n\ninputs['company_n'] = le_company.fit_transform(inputs['company'])\ninputs['job_n'] = le_job.fit_transform(inputs['job'])\ninputs['degree_n'] = le_degree.fit_transform(inputs['degree'])\n\n\ninputs\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\ncompany_n\njob_n\ndegree_n\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n2\n2\n0\n\n\n1\ngoogle\nsales executive\nmasters\n2\n2\n1\n\n\n2\ngoogle\nbusiness manager\nbachelors\n2\n0\n0\n\n\n3\ngoogle\nbusiness manager\nmasters\n2\n0\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n2\n1\n0\n\n\n5\ngoogle\ncomputer programmer\nmasters\n2\n1\n1\n\n\n6\nabc pharma\nsales executive\nmasters\n0\n2\n1\n\n\n7\nabc pharma\ncomputer programmer\nbachelors\n0\n1\n0\n\n\n8\nabc pharma\nbusiness manager\nbachelors\n0\n0\n0\n\n\n9\nabc pharma\nbusiness manager\nmasters\n0\n0\n1\n\n\n10\nfacebook\nsales executive\nbachelors\n1\n2\n0\n\n\n11\nfacebook\nsales executive\nmasters\n1\n2\n1\n\n\n12\nfacebook\nbusiness manager\nbachelors\n1\n0\n0\n\n\n13\nfacebook\nbusiness manager\nmasters\n1\n0\n1\n\n\n14\nfacebook\ncomputer programmer\nbachelors\n1\n1\n0\n\n\n15\nfacebook\ncomputer programmer\nmasters\n1\n1\n1\n\n\n\n\n\n\n\n\ninputs_n = inputs.drop(['company','job','degree'],axis='columns')\n\n\ninputs_n\n\n\n\n\n\n\n\n\ncompany_n\njob_n\ndegree_n\n\n\n\n\n0\n2\n2\n0\n\n\n1\n2\n2\n1\n\n\n2\n2\n0\n0\n\n\n3\n2\n0\n1\n\n\n4\n2\n1\n0\n\n\n5\n2\n1\n1\n\n\n6\n0\n2\n1\n\n\n7\n0\n1\n0\n\n\n8\n0\n0\n0\n\n\n9\n0\n0\n1\n\n\n10\n1\n2\n0\n\n\n11\n1\n2\n1\n\n\n12\n1\n0\n0\n\n\n13\n1\n0\n1\n\n\n14\n1\n1\n0\n\n\n15\n1\n1\n1\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nnp.random.seed(42)\nn_points = len(inputs_n)\nx = np.random.rand(n_points)\ny = np.random.rand(n_points)\nz = np.random.rand(n_points)\n\n\ninputs_n['company_n'] += (x-0.5)/10\ninputs_n['job_n'] += (y-0.5)/10\ninputs_n['degree_n'] += (z-0.5)/10\n\n\n# Create a 3D scatter plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.set_xlabel('degree_n')\nax.set_ylabel('job_n')\nax.set_zlabel('company_n')\n\nscatter = ax.scatter(inputs_n['degree_n'],\n           inputs_n['job_n'],\n           inputs_n['company_n'],\n           c=target,\n           cmap='viridis',\n           marker='+')\n\n# Adding a color bar to show the mapping of colors to values in 'color_column'\ncbar = fig.colorbar(scatter, ax=ax)\ncbar.set_label('Color Column')\n\n\n\n\n\ntarget\n\n0     0\n1     0\n2     1\n3     1\n4     0\n5     1\n6     0\n7     0\n8     0\n9     1\n10    1\n11    1\n12    1\n13    1\n14    1\n15    1\nName: salary_more_then_100k, dtype: int64\n\n\n\nfrom sklearn import tree\n\n\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(inputs_n, target)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\nmodel.score(inputs_n,target)\n\n1.0\n\n\n\nfrom sklearn.tree import export_graphviz\n\n\nFEATURE_NAMES = ['company_n', 'job_n', 'degree_n']\nexport_graphviz(model, './Data/salary.dot', feature_names = FEATURE_NAMES)\n\n\n!dot -Tpng ./Data/salary.dot -o ./Data/salary.png\n\n\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n\nimg = cv.imread('./Data/salary.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)\n\n&lt;matplotlib.image.AxesImage&gt;"
  },
  {
    "objectID": "decisiontree.html#predict",
    "href": "decisiontree.html#predict",
    "title": "Decision Tree",
    "section": "Predict",
    "text": "Predict\n\nmodel.predict([[2,1,0]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\n\nmodel.predict([[2,1,1]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])"
  },
  {
    "objectID": "ensemble.html",
    "href": "ensemble.html",
    "title": "Ensemble Methods: Bagging and Boosting",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "cnns.html",
    "href": "cnns.html",
    "title": "Convolution Neutal Networks",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#step-for-git-push",
    "href": "index.html#step-for-git-push",
    "title": "ML",
    "section": "Step for git push",
    "text": "Step for git push\n\nnbdev_prepare\n\nnbdev_prepare\n\nGit stuff\n\ngit add .\ngit commit -m \"update\"\ngit push"
  },
  {
    "objectID": "index.html#after-changing-dependencies",
    "href": "index.html#after-changing-dependencies",
    "title": "ML",
    "section": "After changing dependencies",
    "text": "After changing dependencies\npip install ML\npip install -e '.[dev]'"
  },
  {
    "objectID": "backpropagation.html",
    "href": "backpropagation.html",
    "title": "Back Propagation",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "generative.html",
    "href": "generative.html",
    "title": "Generative Models and Learning from Unlabelled Data",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "k-nn.html",
    "href": "k-nn.html",
    "title": "K-NN",
    "section": "",
    "text": "def sayhello(name): return f'Hello {name}'"
  },
  {
    "objectID": "k-nn.html#advantages",
    "href": "k-nn.html#advantages",
    "title": "K-NN",
    "section": "Advantages",
    "text": "Advantages\n\nEasy to implement: Given the algorithm’s simplicity and accuracy, it is one of the first classifiers that a new data scientist will learn.\nAdapts easily: As new training samples are added, the algorithm adjusts to account for any new data since all training data is stored into memory.\nFew hyperparameters: KNN only requires a k value and a distance metric, which is low when compared to other machine learning algorithms."
  },
  {
    "objectID": "k-nn.html#disadvantages",
    "href": "k-nn.html#disadvantages",
    "title": "K-NN",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nDoes not scale well: Since KNN is a lazy algorithm, it takes up more memory and data storage compared to other classifiers. This can be costly from both a time and money perspective. More memory and storage will drive up business expenses and more data can take longer to compute. While different data structures, such as Ball-Tree, have been created to address the computational inefficiencies, a different classifier may be ideal depending on the business problem.\nCurse of dimensionality: The KNN algorithm tends to fall victim to the curse of dimensionality, which means that it doesn’t perform well with high-dimensional data inputs. This is sometimes also referred to as the peaking phenomenon (PDF, 340 MB) (link resides outside of ibm.com), where after the algorithm attains the optimal number of features, additional features increases the amount of classification errors, especially when the sample size is smaller.\nProne to overfitting: Due to the “curse of dimensionality”, KNN is also more prone to overfitting. While feature selection and dimensionality reduction techniques are leveraged to prevent this from occurring, the value of k can also impact the model’s behavior. Lower values of k can overfit the data, whereas higher values of k tend to “smooth out” the prediction values since it is averaging the values over a greater area, or neighborhood. However, if the value of k is too high, then it can underfit the data."
  },
  {
    "objectID": "k-nn.html#distance-metrics",
    "href": "k-nn.html#distance-metrics",
    "title": "K-NN",
    "section": "Distance Metrics",
    "text": "Distance Metrics\n\nEuclidean distance (p=2): This is the most commonly used distance measure\nManhattan distance (p=1): This is also another popular distance metric, which measures the absolute value between two points.\nMinkowski distance: This distance measure is the generalized form of Euclidean and Manhattan distance metrics.\nHamming distance: This technique is used typically used with Boolean or string vectors, identifying the points where the vectors do not match."
  },
  {
    "objectID": "k-nn.html#compute-knn-defining-k",
    "href": "k-nn.html#compute-knn-defining-k",
    "title": "K-NN",
    "section": "Compute KNN: defining k",
    "text": "Compute KNN: defining k\nThe k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
  },
  {
    "objectID": "k-nn.html#data-collection",
    "href": "k-nn.html#data-collection",
    "title": "K-NN",
    "section": "Data collection",
    "text": "Data collection\n\n!pip list | grep pandas\n!pip list | grep scikit-learn\n\npandas                        2.0.3\nscikit-learn                  1.3.0\n\n\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n\niris = load_iris()\n\n\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\niris.target\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ndf['target'] = iris.target\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\ndf[df.target==1].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\n\n\n\n\n\n\n\n\ndf[df.target==2].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2\n\n\n\n\n\n\n\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\nsetosa\n\n\n\n\n\n\n\n\ndf0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]"
  },
  {
    "objectID": "k-nn.html#data-visuals",
    "href": "k-nn.html#data-visuals",
    "title": "K-NN",
    "section": "Data Visuals",
    "text": "Data Visuals\n\nimport matplotlib.pyplot as plt\n\n\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'],\n            df0['sepal width (cm)'],\n            color=\"green\",marker='+')\n\nplt.scatter(df1['sepal length (cm)'],\n            df1['sepal width (cm)'],\n            color=\"blue\",marker='.')\n\nplt.scatter(df2['sepal length (cm)'],\n            df2['sepal width (cm)'],\n            color=\"red\",marker='*')\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\n\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'],\n            df0['petal width (cm)'],\n            color=\"green\",marker='+')\n\nplt.scatter(df1['petal length (cm)'],\n            df1['petal width (cm)'],\n            color=\"blue\",marker='.')\n\nplt.scatter(df2['sepal length (cm)'],\n            df2['sepal width (cm)'],\n            color=\"red\",marker='*')\n\n&lt;matplotlib.collections.PathCollection&gt;"
  },
  {
    "objectID": "k-nn.html#model-design",
    "href": "k-nn.html#model-design",
    "title": "K-NN",
    "section": "Model Design",
    "text": "Model Design\n\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.2,\n                                                    random_state=1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nlen(X_train)\n\n120\n\n\n\nlen(X_test)\n\n30"
  },
  {
    "objectID": "k-nn.html#first-model-k-nn-10",
    "href": "k-nn.html#first-model-k-nn-10",
    "title": "K-NN",
    "section": "First Model: K-nn 10",
    "text": "First Model: K-nn 10\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nn_neighbors=10\nknn = KNeighborsClassifier(n_neighbors)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\n\nknn.score(X_test, y_test)\n\n1.0\n\n\n\nknn.predict([[4.8,3.0,1.5,0.3]])\n\narray([2])\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm\n\narray([[11,  0,  0],\n       [ 0, 13,  0],\n       [ 0,  0,  6]])\n\n\n\n!pip list | grep seaborn || pip install seaborn\n\nseaborn                       0.12.2\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(58.222222222222214, 0.5, 'Truth')\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        11\n           1       1.00      1.00      1.00        13\n           2       1.00      1.00      1.00         6\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\n\n\n# Plot the decision regions\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# Define the resolution of the grid\nh = 0.02\nx_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\ny_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\nx1_min, x1_max = X_train[:, 2].min() - 1, X_train[:, 2].max() + 1\ny1_min, y1_max = X_train[:, 3].min() - 1, X_train[:, 3].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nxx1, yy1 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(y1_min, y1_max, h))\n\n\n# Predict the class labels for each point in the grid\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel(),xx.ravel(), yy.ravel() ])\nZ = Z.reshape(xx.shape)\n\nprint(f'{len(xx)} {len(yy)} {len(xx1)} {len(yy1)} ')\n\n387 387 256 256 \n\n\n\n# Plot the decision regions\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot the training points\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classifier with k={}\".format(10))\n\n\nplt.show()\n\n\n\n\n\nlen(xx1.ravel())\n\n68608\n\n\n\n# Plot the decision regions\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot the training points\nplt.scatter(X_train[:, 2], X_train[:, 3], c=y_train, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classifier with k={}\".format(10))\n\nText(0.5, 1.0, 'k-NN classifier with k=10')"
  },
  {
    "objectID": "k-nn.html#knn-model-with-gridsearchcv-finding-optimum-k",
    "href": "k-nn.html#knn-model-with-gridsearchcv-finding-optimum-k",
    "title": "K-NN",
    "section": "Knn model with GridSearchCV: finding optimum K",
    "text": "Knn model with GridSearchCV: finding optimum K\n\nfrom sklearn.model_selection import GridSearchCV\n\n\ncreate a dictionary of all values we want to test for n_neighbors\n\n\nknn2 = KNeighborsClassifier()\nparam_grid = {'n_neighbors': np.arange(1, 25)}\n\n\nuse gridsearch to test all values for n_neighbors\n\n\nknn_gscv = GridSearchCV(knn2, param_grid, cv=5)#fit model to data\nknn_gscv.fit(X, y)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24])})estimator: KNeighborsClassifierKNeighborsClassifier()KNeighborsClassifierKNeighborsClassifier()\n\n\n\n#check top performing n_neighbors value\nknn_gscv.best_params_\n\n{'n_neighbors': 6}"
  },
  {
    "objectID": "07_L1 and L2 Regularization-Copy1.html",
    "href": "07_L1 and L2 Regularization-Copy1.html",
    "title": "L1 and L2 Regularization",
    "section": "",
    "text": "L1 and L2 Regularization\n# import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Suppress Warnings for clean notebook\nimport warnings\nwarnings.filterwarnings('ignore')\nWe are going to use Melbourne House Price Dataset where we’ll predict House Predictions based on various features. #### The Dataset Link is https://www.kaggle.com/anthonypino/melbourne-housing-market\n# read dataset\ndataset_og = pd.read_csv('./Data/Melbourne_housing_FULL.csv')\ndataset_og.head()\n\n\n\n\n\n\n\n\nSuburb\nAddress\nRooms\nType\nPrice\nMethod\nSellerG\nDate\nDistance\nPostcode\n...\nBathroom\nCar\nLandsize\nBuildingArea\nYearBuilt\nCouncilArea\nLattitude\nLongtitude\nRegionname\nPropertycount\n\n\n\n\n0\nAbbotsford\n68 Studley St\n2\nh\nNaN\nSS\nJellis\n3/09/2016\n2.5\n3067.0\n...\n1.0\n1.0\n126.0\nNaN\nNaN\nYarra City Council\n-37.8014\n144.9958\nNorthern Metropolitan\n4019.0\n\n\n1\nAbbotsford\n85 Turner St\n2\nh\n1480000.0\nS\nBiggin\n3/12/2016\n2.5\n3067.0\n...\n1.0\n1.0\n202.0\nNaN\nNaN\nYarra City Council\n-37.7996\n144.9984\nNorthern Metropolitan\n4019.0\n\n\n2\nAbbotsford\n25 Bloomburg St\n2\nh\n1035000.0\nS\nBiggin\n4/02/2016\n2.5\n3067.0\n...\n1.0\n0.0\n156.0\n79.0\n1900.0\nYarra City Council\n-37.8079\n144.9934\nNorthern Metropolitan\n4019.0\n\n\n3\nAbbotsford\n18/659 Victoria St\n3\nu\nNaN\nVB\nRounds\n4/02/2016\n2.5\n3067.0\n...\n2.0\n1.0\n0.0\nNaN\nNaN\nYarra City Council\n-37.8114\n145.0116\nNorthern Metropolitan\n4019.0\n\n\n4\nAbbotsford\n5 Charles St\n3\nh\n1465000.0\nSP\nBiggin\n4/03/2017\n2.5\n3067.0\n...\n2.0\n0.0\n134.0\n150.0\n1900.0\nYarra City Council\n-37.8093\n144.9944\nNorthern Metropolitan\n4019.0\n\n\n\n\n5 rows × 21 columns\ndataset_og.nunique()\n\nSuburb             351\nAddress          34009\nRooms               12\nType                 3\nPrice             2871\nMethod               9\nSellerG            388\nDate                78\nDistance           215\nPostcode           211\nBedroom2            15\nBathroom            11\nCar                 15\nLandsize          1684\nBuildingArea       740\nYearBuilt          160\nCouncilArea         33\nLattitude        13402\nLongtitude       14524\nRegionname           8\nPropertycount      342\ndtype: int64\n# let's use limited columns which makes more sense for serving our purpose\ncols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', \n               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']\ndataset = dataset_og[cols_to_use]\ndataset.head()\n\n\n\n\n\n\n\n\nSuburb\nRooms\nType\nMethod\nSellerG\nRegionname\nPropertycount\nDistance\nCouncilArea\nBedroom2\nBathroom\nCar\nLandsize\nBuildingArea\nPrice\n\n\n\n\n0\nAbbotsford\n2\nh\nSS\nJellis\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n1.0\n126.0\nNaN\nNaN\n\n\n1\nAbbotsford\n2\nh\nS\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n1.0\n202.0\nNaN\n1480000.0\n\n\n2\nAbbotsford\n2\nh\nS\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n0.0\n156.0\n79.0\n1035000.0\n\n\n3\nAbbotsford\n3\nu\nVB\nRounds\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n3.0\n2.0\n1.0\n0.0\nNaN\nNaN\n\n\n4\nAbbotsford\n3\nh\nSP\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n3.0\n2.0\n0.0\n134.0\n150.0\n1465000.0\ndataset.shape\n\n(34857, 15)"
  },
  {
    "objectID": "07_L1 and L2 Regularization-Copy1.html#normal-regression-is-clearly-overfitting-the-data-lets-try-other-models",
    "href": "07_L1 and L2 Regularization-Copy1.html#normal-regression-is-clearly-overfitting-the-data-lets-try-other-models",
    "title": "L1 and L2 Regularization",
    "section": "Normal Regression is clearly overfitting the data, let’s try other models",
    "text": "Normal Regression is clearly overfitting the data, let’s try other models\n\nUsing Lasso (L1 Regularized) Regression Model\n\nfrom sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=50, max_iter=100, tol=0.1)\nlasso_reg.fit(train_X, train_y)\n\nLasso(alpha=50, max_iter=100, tol=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=50, max_iter=100, tol=0.1)\n\n\n\nlasso_reg.score(test_X, test_y)\n\n0.6636111369404488\n\n\n\nlasso_reg.score(train_X, train_y)\n\n0.6766985624766824\n\n\n\nUsing Ridge (L2 Regularized) Regression Model\n\nfrom sklearn.linear_model import Ridge\nridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)\nridge_reg.fit(train_X, train_y)\n\nRidge(alpha=50, max_iter=100, tol=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50, max_iter=100, tol=0.1)\n\n\n\nridge_reg.score(test_X, test_y)\n\n0.6670848945194958\n\n\n\nridge_reg.score(train_X, train_y)\n\n0.6622376739684328\n\n\nWe see that Lasso and Ridge Regularizations prove to be beneficial when our Simple Linear Regression Model overfits. These results may not be that contrast but significant in most cases.Also that L1 & L2 Regularizations are used in Neural Networks too"
  },
  {
    "objectID": "loss_or_cost_funtions-copy1.html",
    "href": "loss_or_cost_funtions-copy1.html",
    "title": "Back Propagation and Gradient Descent",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "polynomialregression.html",
    "href": "polynomialregression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nX = np.random.rand(100,1)\ny = 4 + 5 * X + 1 * np.random.randn(100, 1)\nplt.scatter(X, y)\nplt.show()\nreg = LinearRegression()\nreg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nX_vals = np.linspace(0, 1, 100).reshape(-1,1)\ny_vals = reg.predict(X_vals)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()"
  },
  {
    "objectID": "polynomialregression.html#second-order",
    "href": "polynomialregression.html#second-order",
    "title": "Polynomial Regression",
    "section": "Second Order",
    "text": "Second Order\n\nX = 4 * np.random.rand(50,1) -2\ny = 4 + 2 * X + 5 * X ** 2 + 2 * np.random.randn(50, 1)\n\n\npoly_features = PolynomialFeatures(degree = 2, include_bias = False)\nX_poly = poly_features.fit_transform(X)\n\n\nreg1 = LinearRegression()\nreg1.fit(X_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nX_vals = np.linspace(-2, 2, 50).reshape(-1,1)\nX_vals_poly = poly_features.transform(X_vals)\ny_vals = reg1.predict(X_vals_poly)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()\n\n\n\n\n\nX_vals_poly[:,1]\n\narray([4.00000000e+00, 3.68013328e+00, 3.37359434e+00, 3.08038317e+00,\n       2.80049979e+00, 2.53394419e+00, 2.28071637e+00, 2.04081633e+00,\n       1.81424406e+00, 1.60099958e+00, 1.40108288e+00, 1.21449396e+00,\n       1.04123282e+00, 8.81299459e-01, 7.34693878e-01, 6.01416077e-01,\n       4.81466056e-01, 3.74843815e-01, 2.81549354e-01, 2.01582674e-01,\n       1.34943773e-01, 8.16326531e-02, 4.16493128e-02, 1.49937526e-02,\n       1.66597251e-03, 1.66597251e-03, 1.49937526e-02, 4.16493128e-02,\n       8.16326531e-02, 1.34943773e-01, 2.01582674e-01, 2.81549354e-01,\n       3.74843815e-01, 4.81466056e-01, 6.01416077e-01, 7.34693878e-01,\n       8.81299459e-01, 1.04123282e+00, 1.21449396e+00, 1.40108288e+00,\n       1.60099958e+00, 1.81424406e+00, 2.04081633e+00, 2.28071637e+00,\n       2.53394419e+00, 2.80049979e+00, 3.08038317e+00, 3.37359434e+00,\n       3.68013328e+00, 4.00000000e+00])\n\n\n\nplt.scatter(X_vals, X_vals_poly[:,1])\n\n&lt;matplotlib.collections.PathCollection&gt;"
  },
  {
    "objectID": "polynomialregression.html#higher-order",
    "href": "polynomialregression.html#higher-order",
    "title": "Polynomial Regression",
    "section": "Higher Order",
    "text": "Higher Order\n\nX = 4 * np.random.rand(50,1) -2\ny = 4 + 2 * X + 5 * X ** 2 + 12 * X ** 3 + 2 * X ** 4 + + 2 * np.random.randn(50, 1)\n\npoly_features = PolynomialFeatures(degree = 4, include_bias = False)\nX_poly = poly_features.fit_transform(X)\n\nreg2 = LinearRegression()\nreg2.fit(X_poly, y)\n\nX_vals = np.linspace(-2, 2, 50).reshape(-1,1)\nX_vals_poly = poly_features.transform(X_vals)\ny_vals = reg2.predict(X_vals_poly)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()"
  },
  {
    "objectID": "linearregression_multivariate.html",
    "href": "linearregression_multivariate.html",
    "title": "Linear Regression Multiple Variables",
    "section": "",
    "text": "Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.\n\nGiven these home prices find out price of a home that has,\n3000 sqr ft area, 3 bedrooms, 40 year old\n2500 sqr ft area, 4 bedrooms, 5 year old\nWe will use regression with multiple variables here. Price can be calculated using following equation,\n\nHere area, bedrooms, age are called independant variables or features whereas price is a dependant variable\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\n\ndf = pd.read_csv('./Data/homeprices.csv')\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\nData Preprocessing: Fill NA values with median value of a column\n\ndf.bedrooms.median()\n\n4.0\n\n\n\ndf.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\n4.0\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nreg = linear_model.LinearRegression()\nreg.fit(df.drop('price',axis='columns'),df.price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nreg.coef_\n\narray([  112.06244194, 23388.88007794, -3231.71790863])\n\n\n\nreg.intercept_\n\n221323.00186540408\n\n\nFind price of home with 3000 sqr ft area, 3 bedrooms, 40 year old\n\nreg.predict([[3000, 3, 40]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([498408.25158031])\n\n\n\n112.06244194*3000 + 23388.88007794*3 + -3231.71790863*40 + 221323.00186540384\n\n498408.25157402386\n\n\nFind price of home with 2500 sqr ft area, 4 bedrooms, 5 year old\n\nreg.predict([[2500, 4, 5]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([578876.03748933])\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\ncorrcoef = np.corrcoef(df, rowvar=False)\ncorrcoef\n\narray([[ 1.  ,  0.75, -0.45,  0.9 ],\n       [ 0.75,  1.  , -0.88,  0.92],\n       [-0.45, -0.88,  1.  , -0.73],\n       [ 0.9 ,  0.92, -0.73,  1.  ]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndf.columns\n\nIndex(['area', 'bedrooms', 'age', 'price'], dtype='object')\n\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(corrcoef, annot=True)\n\nplt.xticks(range(len(df.columns)), df.columns)\nplt.yticks(range(len(df.columns)), df.columns)\n# Move x-axis ticks and labels to the top\nplt.gca().xaxis.set_ticks_position('top')\n\n\nplt.show()"
  },
  {
    "objectID": "linearregression_multivariate.html#sample-problem-of-predicting-home-price-in-monroe-new-jersey-usa",
    "href": "linearregression_multivariate.html#sample-problem-of-predicting-home-price-in-monroe-new-jersey-usa",
    "title": "Linear Regression Multiple Variables",
    "section": "",
    "text": "Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.\n\nGiven these home prices find out price of a home that has,\n3000 sqr ft area, 3 bedrooms, 40 year old\n2500 sqr ft area, 4 bedrooms, 5 year old\nWe will use regression with multiple variables here. Price can be calculated using following equation,\n\nHere area, bedrooms, age are called independant variables or features whereas price is a dependant variable\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\n\ndf = pd.read_csv('./Data/homeprices.csv')\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\nData Preprocessing: Fill NA values with median value of a column\n\ndf.bedrooms.median()\n\n4.0\n\n\n\ndf.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\n4.0\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nreg = linear_model.LinearRegression()\nreg.fit(df.drop('price',axis='columns'),df.price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nreg.coef_\n\narray([  112.06244194, 23388.88007794, -3231.71790863])\n\n\n\nreg.intercept_\n\n221323.00186540408\n\n\nFind price of home with 3000 sqr ft area, 3 bedrooms, 40 year old\n\nreg.predict([[3000, 3, 40]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([498408.25158031])\n\n\n\n112.06244194*3000 + 23388.88007794*3 + -3231.71790863*40 + 221323.00186540384\n\n498408.25157402386\n\n\nFind price of home with 2500 sqr ft area, 4 bedrooms, 5 year old\n\nreg.predict([[2500, 4, 5]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([578876.03748933])\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\ncorrcoef = np.corrcoef(df, rowvar=False)\ncorrcoef\n\narray([[ 1.  ,  0.75, -0.45,  0.9 ],\n       [ 0.75,  1.  , -0.88,  0.92],\n       [-0.45, -0.88,  1.  , -0.73],\n       [ 0.9 ,  0.92, -0.73,  1.  ]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndf.columns\n\nIndex(['area', 'bedrooms', 'age', 'price'], dtype='object')\n\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(corrcoef, annot=True)\n\nplt.xticks(range(len(df.columns)), df.columns)\nplt.yticks(range(len(df.columns)), df.columns)\n# Move x-axis ticks and labels to the top\nplt.gca().xaxis.set_ticks_position('top')\n\n\nplt.show()"
  },
  {
    "objectID": "logisticregression.html",
    "href": "logisticregression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Binary Classification\nMulticlass Classification"
  },
  {
    "objectID": "logisticregression.html#classification-types",
    "href": "logisticregression.html#classification-types",
    "title": "Logistic Regression",
    "section": "",
    "text": "Binary Classification\nMulticlass Classification"
  },
  {
    "objectID": "logisticregression.html#binary-classification",
    "href": "logisticregression.html#binary-classification",
    "title": "Logistic Regression",
    "section": "Binary Classification",
    "text": "Binary Classification\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nimport numpy as np\n\n\npath = Path('Data/insurance_data.csv')\ndf = pd.read_csv(path)\ndf.head()\n\n\n\n\n\n\n\n\nage\nbought_insurance\n\n\n\n\n0\n22\n0\n\n\n1\n25\n0\n\n\n2\n47\n1\n\n\n3\n52\n0\n\n\n4\n46\n1\n\n\n\n\n\n\n\n\\(\\text{sigmoid}(z) = \\dfrac{1}{1+e^-z}\\) where e = Euler’s number ~ 2.71828\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\n\n\nX_test\n\n\n\n\n\n\n\n\nage\n\n\n\n\n12\n27\n\n\n25\n54\n\n\n7\n60\n\n\n2\n47\n\n\n10\n18\n\n\n9\n61\n\n\n\n\n\n\n\n\nmodel = LogisticRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nX_test\n\n\n\n\n\n\n\n\nage\n\n\n\n\n12\n27\n\n\n25\n54\n\n\n7\n60\n\n\n2\n47\n\n\n10\n18\n\n\n9\n61\n\n\n\n\n\n\n\n\ny_predicted = model.predict(X_test)\ny_predicted\n\narray([0, 1, 1, 1, 0, 1])\n\n\n\ny_probability = model.predict_proba(X_test)\n\n\nmodel.score(X_test,y_test)\n\n1.0\n\n\n\narray1 = np.array(X_test)\n\n# Stack the arrays horizontally\ncombined_array = np.hstack((array1, y_probability[:,1].reshape(-1, 1)))\n\nprint(\"Combined Array:\")\nprint(combined_array)\n\nCombined Array:\n[[27.          0.18888707]\n [54.          0.84143616]\n [60.          0.91401439]\n [47.          0.70233749]\n [18.          0.07590529]\n [61.          0.92268871]]\n\n\n\nsorted_array = np.sort(combined_array, 0)\nsorted_array[:,0]\n\narray([18., 27., 47., 54., 60., 61.])\n\n\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='blue')\nplt.scatter(X_test,y_test,marker='+',color='red')\nplt.plot(sorted_array[:,0],sorted_array[:,1],marker='+',color='green')\nplt.scatter(X_test,y_predicted,marker='*',color='green')\n\n&lt;matplotlib.collections.PathCollection&gt;"
  },
  {
    "objectID": "logisticregression.html#multiclass-regression",
    "href": "logisticregression.html#multiclass-regression",
    "title": "Logistic Regression",
    "section": "Multiclass Regression",
    "text": "Multiclass Regression\n\nfrom sklearn.datasets import load_digits\nimport matplotlib.pyplot as plt\n\n\ndigits = load_digits()\n\n\nplt.gray() \nfor i in range(2):\n    plt.matshow(digits.images[i])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\ndir(digits)\n\n['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n\n\n\ndigits.target[:]\n\narray([0, 1, 2, ..., 8, 9, 8])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = LogisticRegression()\n\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target, test_size=0.2)\n\n\nmodel.fit(X_train, y_train)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nmodel.score(X_test, y_test)\n\n0.9722222222222222\n\n\n\nmodel.predict(digits.data[0:5])\n\narray([0, 1, 2, 3, 4])\n\n\n\ny_predicted = model.predict(X_test)\n\narray([5, 4, 0, 2, 9, 5, 3, 2, 0, 4, 1, 3, 5, 1, 5, 3, 6, 3, 5, 2, 3, 2,\n       0, 8, 1, 9, 6, 7, 0, 8, 9, 4, 5, 7, 2, 4, 4, 4, 8, 3, 7, 8, 3, 6,\n       4, 9, 2, 4, 6, 3, 5, 1, 6, 0, 7, 9, 4, 8, 8, 3, 8, 9, 5, 6, 4, 9,\n       8, 5, 2, 0, 7, 7, 6, 2, 5, 8, 9, 5, 7, 5, 5, 4, 4, 8, 9, 8, 9, 2,\n       1, 0, 7, 4, 8, 6, 3, 3, 3, 8, 1, 1, 5, 6, 7, 6, 1, 7, 2, 8, 1, 5,\n       3, 4, 4, 9, 5, 0, 7, 0, 6, 3, 2, 2, 4, 3, 4, 8, 6, 0, 8, 0, 3, 1,\n       4, 9, 0, 3, 2, 9, 9, 6, 7, 8, 4, 6, 8, 6, 9, 0, 4, 9, 7, 6, 8, 3,\n       9, 6, 0, 7, 1, 7, 2, 5, 2, 3, 3, 8, 0, 0, 9, 4, 4, 5, 9, 0, 8, 8,\n       7, 9, 9, 8, 3, 3, 8, 7, 0, 4, 6, 6, 1, 1, 9, 0, 3, 1, 3, 9, 2, 8,\n       3, 7, 4, 5, 5, 7, 2, 1, 9, 5, 5, 7, 9, 1, 9, 1, 7, 6, 5, 1, 6, 7,\n       5, 6, 7, 2, 9, 4, 9, 0, 8, 3, 3, 6, 0, 1, 3, 3, 9, 6, 1, 5, 1, 6,\n       6, 3, 1, 0, 1, 0, 2, 2, 1, 9, 7, 9, 1, 0, 9, 1, 3, 8, 1, 5, 0, 0,\n       8, 6, 1, 2, 6, 6, 9, 5, 3, 6, 3, 8, 9, 8, 6, 9, 7, 2, 8, 5, 9, 6,\n       9, 7, 3, 7, 4, 3, 2, 1, 5, 8, 0, 8, 6, 6, 7, 5, 6, 6, 4, 6, 3, 7,\n       2, 3, 6, 8, 5, 3, 1, 6, 8, 8, 9, 0, 8, 5, 6, 8, 0, 1, 2, 0, 0, 1,\n       9, 6, 7, 6, 3, 2, 0, 5, 5, 2, 7, 1, 6, 4, 6, 0, 2, 5, 2, 0, 7, 6,\n       9, 6, 1, 9, 1, 1, 0, 0])\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\n\ncm = confusion_matrix(y_test, y_predicted)\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(95.72222222222221, 0.5, 'Truth')\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nreport = classification_report(y_test, y_predicted)\nprint(report)\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99        35\n           1       0.97      0.97      0.97        36\n           2       0.93      0.96      0.95        28\n           3       0.97      0.95      0.96        40\n           4       0.96      1.00      0.98        27\n           5       0.94      0.94      0.94        35\n           6       1.00      1.00      1.00        46\n           7       1.00      0.97      0.98        33\n           8       0.97      0.97      0.97        38\n           9       0.98      0.95      0.96        42\n\n    accuracy                           0.97       360\n   macro avg       0.97      0.97      0.97       360\nweighted avg       0.97      0.97      0.97       360"
  },
  {
    "objectID": "randomforest.html",
    "href": "randomforest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "crossentropy.html",
    "href": "crossentropy.html",
    "title": "Cross Entropy",
    "section": "",
    "text": "Back to top"
  }
]